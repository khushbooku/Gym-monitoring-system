{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "from google.colab import files\n",
    "from google.colab.patches import cv2_imshow\n",
    "from IPython.display import HTML\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It may take a while...\n",
    "#!wget \"https://documents.epfl.ch/groups/c/cv/cvlab-pom-video1/www/campus4-c0.avi\"\n",
    "from google.colab import files\n",
    "uploaded=files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video(path):\n",
    "    '''Display video in Colab.'''\n",
    "    compressed_path = path.split('.')[0]\n",
    "    compressed_path = 'compressed_' + compressed_path + '.mp4'\n",
    "\n",
    "    if os.path.exists(compressed_path):\n",
    "        os.remove(compressed_path)\n",
    "\n",
    "    # Convert video\n",
    "    os.system(f\"ffmpeg -i {path} -vcodec libx264 {compressed_path}\")\n",
    "\n",
    "    # Show video\n",
    "    mp4 = open(compressed_path,'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(\"\"\"\n",
    "    <video width=400 controls>\n",
    "        <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'campus4-c0.avi'\n",
    "display_video(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "#The model used is YOLOv5x, the best YOLOv5 model. We import it with Torch Hub.\n",
    "\n",
    "#When we pass an image to this model, it returns to us where objects are in the image, which objects they are, and what is the model's confidence about this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5x',\n",
    "                       pretrained=True, verbose=False)\n",
    "model.cuda('cuda:0');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## People detection\n",
    "\n",
    "#The first way to calculate distance among people is just calculate the distance among the rectangles (boxes) returned by the model, more precisely the distance among their centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_distance(xyxy1, xyxy2):\n",
    "    '''Calculate the distance of the centers of the boxes.'''\n",
    "    a, b, c, d = xyxy1\n",
    "    x1 = int(np.mean([a, c]))\n",
    "    y1 = int(np.mean([b, d]))\n",
    "\n",
    "    e, f, g, h = xyxy2\n",
    "    x2 = int(np.mean([e, g]))\n",
    "    y2 = int(np.mean([f, h]))\n",
    "    \n",
    "    dist = np.linalg.norm([x1 - x2, y1 - y2])\n",
    "    return dist, x1, y1, x2, y2\n",
    "#When we have a frame of a video, we can detect the people on the frame using YOLOv5x and draw the rectangles. The color of the rectangle indicates if the person is too close to another person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people_on_frame(img, confidence, distance):\n",
    "    '''Detect people on a frame and draw the rectangles and lines.'''\n",
    "    results = model([img[:, :, ::-1]])  # Pass the frame through the model and get the boxes\n",
    "\n",
    "    xyxy = results.xyxy[0].cpu().numpy()  # xyxy are the box coordinates\n",
    "    #          x1 (pixels)  y1 (pixels)  x2 (pixels)  y2 (pixels)   confidence        class\n",
    "    # tensor([[7.47613e+02, 4.01168e+01, 1.14978e+03, 7.12016e+02, 8.71210e-01, 0.00000e+00],\n",
    "    #         [1.17464e+02, 1.96875e+02, 1.00145e+03, 7.11802e+02, 8.08795e-01, 0.00000e+00],\n",
    "    #         [4.23969e+02, 4.30401e+02, 5.16833e+02, 7.20000e+02, 7.77376e-01, 2.70000e+01],\n",
    "    #         [9.81310e+02, 3.10712e+02, 1.03111e+03, 4.19273e+02, 2.86850e-01, 2.70000e+01]])\n",
    "\n",
    "    xyxy = xyxy[xyxy[:, 4] >= confidence]  # Filter desired confidence\n",
    "    xyxy = xyxy[xyxy[:, 5] == 0]  # Consider only people\n",
    "    xyxy = xyxy[:, :4]\n",
    "\n",
    "    colors = ['green']*len(xyxy)\n",
    "    for i in range(len(xyxy)):\n",
    "        for j in range(i+1, len(xyxy)):\n",
    "            # Calculate distance of the centers\n",
    "            dist, x1, y1, x2, y2 = center_distance(xyxy[i], xyxy[j])\n",
    "            if dist < distance:\n",
    "                # If dist < distance, boxes are red and a line is drawn\n",
    "                colors[i] = 'red'\n",
    "                colors[j] = 'red'\n",
    "                img = cv2.line(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "    for i, (x1, y1, x2, y2) in enumerate(xyxy):\n",
    "        # Draw the boxes\n",
    "        if colors[i] == 'green':\n",
    "            color = (0, 255, 0)\n",
    "        else:\n",
    "            color = (0, 0, 255)\n",
    "        img = cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To detect people in a video, we iterate through all frames of the video, and save a new video with the rectangles drawn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_people_on_video(filename, confidence=0.9, distance=60):\n",
    "    '''Detect people on a video and draw the rectangles and lines.'''\n",
    "    # Capture video\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "\n",
    "    # Get video properties\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    if os.path.exists('output.avi'):\n",
    "        os.remove('output.avi')\n",
    "    out = cv2.VideoWriter('output.avi', fourcc, fps, (width, height))\n",
    "\n",
    "    # Iterate through frames and detect people\n",
    "    vidlen = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    with tqdm(total=vidlen) as pbar:\n",
    "        while cap.isOpened():\n",
    "            # Read a frame\n",
    "            ret, frame = cap.read()\n",
    "            # If it's ok\n",
    "            if ret == True:\n",
    "                frame = detect_people_on_frame(frame, confidence, distance)\n",
    "                # Write new video\n",
    "                out.write(frame)\n",
    "                pbar.update(1)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    # Release everything if job is finished\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's apply the detection tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_people_on_video(filename, confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's watch the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_video('output.avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
